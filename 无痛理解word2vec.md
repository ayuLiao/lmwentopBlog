---
title: 无痛理解word2vec
date: 2017-08-17 09:14:24
tags: NLP
---

## 简介
我们都知道，计算机无法直接处理我们的自然语言，因为不认识，所以NLP中常见的做法就是将自然语言结构化、数字化，这样我们才能通过计算机进行处理，其中比较成熟且常见的做法就是将文章中的词抽象成相应的词向量，抽象成词向量时，需要注意两点

1.词向量可以代表具体的一个词，也就是有区别性，一个词对应一个词向量
2.抽象成词向量时，要保存该词在自然语言中存在的信息

第一点比较好理解，就是创建不同的词向量来存不同的词，第二点的意思就是词向量不能简单的随机生成，而是要有特定的意义，举个简单的例子

一个词，美丽

它有很多同义词，如：漂亮，多姿，靓丽，好看，俊俏，俊丽，迷人，斑斓夺目，五彩斑斓，绚丽多姿，炫丽，绚丽，优美，艳丽，标志，端庄

那么我将美丽这个词抽象成词向量时，如何才能让机器也知道美丽和漂亮有相近的含义

还有点比较重要，就是词向量最好能表示出词出现的顺序概率，举个简单的例子

我爱你 和 你爱我

同样都是这3个字，但是我们知道两句话的含义完全不同，那么怎么才能通过词向量来描述这些词在句子中的不同语序呢？

这里就要提到word2vec了

word2vec是Google在2013年开源的一个将词表特征转换为**实数值**词向量的**高效**工具，简单点说就是将一个词转换为一个词向量，在这个向量中，保存了一些词的特性，比如上面提的这些特性，其实在Google开源word2vec前，工业界也有相关的方法，但是word2vec的特点就是高效，所以word2vec开源后，被大家都拿来用，拿来测试，结果发现效果杠杠的，所以Word2Vec获得的赞誉很高

## Word2Vec简介
Word2Vec利用了简单的一层神经网络把one-hot形式词词向量通过映射层映射为分布式的词向量

要理解这句话，就要有些背景知识

## 背景知识

### one-hot Representation（one-hot表示）
one-hot可以说是最简单的一种词向量表示方式，它通过一个很长的向量来表示一个词，这个向量有多长？一般跟你字典一样长，比如你的字典里有1000个词，那么这个向量长为1000，字典中的每个词，都在相应的位置上标1，其他位置都标0

如：
美丽    [0,0,1,0,0,0,0,0,0,0,....,0]
漂亮	[0,0,0,1,0,0,0,0,0,0,....,0]
好看	[0,0,0,0,1,0,0,0,0,0,....,0]

很简单，很暴力的一种词向量表示方式，其实我们依旧可以使用one-hot形式的词向量进行计算了，可是问题也很明显，比如通过这种方法表示，词与词之间完全就没有关系了，这其实很尴尬，因为词与词之间的关系在NLP处理的时候很重要，可能你听过一句话，输入的是垃圾，不论语言模型有多好，输出的必然是垃圾，所以one-hot这种形式的词向量一般都不用来直接进行计算。

同时它还有其他严重的问题
1.维数灾难，一般公司的语料库都会TB级别的，要创建这么长的向量其实不合适，对内存要求很大同时计算这么大维度的巨稀向量也没什么好处（只有一个地方为一，其他都为0，非常稀疏）
2.词汇鸿沟，就是上面说的，词与词之间没有任何联系，实质上是有一定关系的

所以word2vec对one-hot形式的词向量进行了处理，将其通过各种方法转换成分布式词向量

### Distributed Representation（分散式表示）
Distributed Representation在1986年被提出，提出的目的就是为了解决one-hot表示方法的缺陷，它也是一种词向量的表示方式，只是这种表示方法具有一定的意义，也就是包含了词的一些信息，可以通过这种表示方式判断词与词的相似性，Distributed Representation也算是种稠密的向量的，它一般长这样

如：
美丽	[0.9762,-0,3442,0.3612,0.8832,-0.2314,-0.1123,-0.8723,0.3312]

Distributed Representation可以通过一系列算法计算出来，具有数学上的含义，那么Distributed Representation的维度是多少呢？这个我们可以根据情况自己定义，常见的情况就是50维或100维，维度越高运算量越大，当然，效果就越好

得到一个语料库中所有词的Distributed Representation后，就可以进行词汇相似度的判断了

讲到这里，就明白了word2vec是生成词汇对应Distributed Representation的一种方法，以词的one-hot Representation为输入，以词的Distributed Representation为输出，然后我们就可以通过词的Distributed Representation做下一步运算，比如通过汉明距离或欧式距离来求词与词的相似度

词与词的相似度，可以跨域不同的语种，用于实现机器翻译的功能，举个简单的例子理解一下，比如汉语中的一二三四五这几个数和英语中的one，two，three，four，five是有一定的关系的，一般在汉语中一常使用的位置或一常代表的含义在英语中one也同样使用在类似的位置，代表类似的含义，这些语言上的信息会体现在语料库中，通过word2vec计算出语言库中所有词的Distributed Representation后，这些词，中文中的一二三四五和英文中的one,two,three,four,five在**空间上分别具有相似性**（向量当然可以在空间上分布）

Google的TomasMikolov 团队将不同语种的词汇转换成词向量，然后用于做翻译，试验中英语和西班牙间的翻译准确率高达90%，在他们讲解他们的实现原理时，有个图可以看一下

![向量空间词汇相似.jpg](http://obfs4iize.bkt.clouddn.com/%E5%90%91%E9%87%8F%E7%A9%BA%E9%97%B4%E8%AF%8D%E6%B1%87%E7%9B%B8%E4%BC%BC.jpg)

图中是他们训练后得到对应的词向量空间，当然对它进行了降维处理，人最多只能理解3维的空间，所以为了理解，就做了降维处理

可以发现英文的one~five和西班牙语的uno~cinco在向量分布上有很大的相似性，所以可以做机器翻译

其实也不能理解，对应机器来说，它根本不知道这是英语还是汉语还是西班牙语，对应它来说，都是数字，然后通过算法，计算出了对应的词向量，算法考虑了词的各种关系，那么得到词向量在不同语言的相似词汇上有很大的相似性其实并不奇怪

这里需要提一下，现在2017，翻译率90%算不怎么优秀的了，人进行翻译，其准确率在96%~98%直接，要达到这种翻译率，还需要注意很多事情，也是现在的挑战之一


那么我们明白了word2vec的输入和输出，接着就来搞明白中间发生了什么


## word2vec原理

Word2Vec将one-hot变成Distributed Representation，这中间Word2Vec做了什么操作？

一图胜千言，看Word2vec原生论文中的图

![word2vec.jpg](http://obfs4iize.bkt.clouddn.com/word2vec.jpg)

（接着解释图）