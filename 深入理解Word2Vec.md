---
title: 深入理解Word2Vec
date: 2017-11-26 09:33:21
tags: 网络协议
---


## 简介
Word2vec是将词转换为向量的一种方法，在NLP中使用非常广泛。

首先要明白，对于计算机来说，无论是中文还在英文，都是一堆0和1，没有什么意义，那么为了让计算机明白我们语言中的意义，就需要将语言抽象化、数字化，让语言就算变成了0和1也具有原本的意义，这就是word2vec要做的事情

## 语言数字化
将语言抽象成具体的向量一般是NLP中要做的第一步，就像一个翻译的过程，将人类的语言翻译给计算机，让计算机明白，问题就在于，当语言中的词抽象成一个具体的向量时如何保存其中的含义，这点比较困难，我们从简单的一步步的说起

首先最简单一种转换形式就是**one-hot Representation**，独热表示方式，**它通过一个很长的向量来表示一个个具体的词**，一般这个向量的长度与词典的一样，这样才能将词典中的每个词都通过不重复的向量表示出来，比如你的字典里有1000个词，那么这个向量长为1000，字典中的每个词，都在相应的位置上标1，其他位置都标0


```
美丽 [0,0,1,0,0,0,0,0,0,0,….,0]
漂亮 [0,0,0,1,0,0,0,0,0,0,….,0]
好看 [0,0,0,0,1,0,0,0,0,0,….,0]
```

这种方法很简单暴力将语言中的词翻译为了计算机可以理解的向量，我们可以使用one-hot形式的词向量进行计算，但是问题也很明显，可以看出，通过这种简单的方式表示一个词，词与词之间就完全没有关系了，从上面的例子也可以看出，其实美丽、漂亮和好看都是意义相近的词，可通过one-hot表示后，对于计算机来说它们就是完成不同的词了，而且有些词可以在不同的语境下有不同的含义，这个one-hot的表示也无能为力，更别说中华文化博大精深，动不动就一语双关了

简单总结一下，就是下面的两个问题：

**1.维数灾难，一般公司的语料库都会TB级别的，要创建这么长的向量其实不合适，对内存要求很大同时计算这么大维度的巨稀向量也没什么好处（只有一个地方为一，其他都为0，非常稀疏）**

**2.词汇鸿沟，就是上面说的，词与词之间没有任何联系，实质上是有一定关系的**

那有没有方法可以解决上面的问题呢？有，那就是Distributed Representation,分散式表示方式，如何理解？其实非常简单，就是将一件事情通过不同的特征表示出来，举个具体的例子

```
红色的大型卡车    [1,0,0,0,0...0]
白色的中型轿车    [0,1,0,0,0...0]
蓝色的小型电动车  [0,0,1,0,0...0]
```

如果用one-hot的表示方式，每一句话就需要单独的一个向量来表示，这种方法会造成维度灾难和词汇鸿沟的问题，那么使用分散式的表示方法可以将上面三句话通过颜色、型号和车型来表示

```
一句话 = 颜色 x 型号 x 车型
```

这样就只需将颜色、型号和车型表示成词向量，而不必每一句类似的话都要通过单独的词向量来表示，节省的空间，同时也保存这几句话之间的关系， 每句话都有颜色、型号和车型，说明这几句话很相近

Distributed Representation表示一句话可以理解，那如何表示一个词呢？这里就可以引出现代统计自然语言处理中最有创建的想法之一（敲黑板）：**用一个词附近的其他词可以表示该词**，可能比较抽象，举个具体的例子就是，如果我不了解你，那么我可以通过你周围的朋友来定义你是个什么样的人，有什么兴趣爱好，从而了解到你，词也一样，我不知道这个词有什么含义，那么我可以通过经常与你一起出现的几个词来明白该词的含义，这也是Distributed Representation可以表示一个词的核心理念

但这也带来了一些问题，比如，该词周围的多少个词才能描述它呢？随着语料的增大，可以描述该词的词很多，一样会有维度过高的问题，那么最直接的想法就是通过SVD对维度过高的向量矩阵进行降维处理同时保留其中的信息，但是**通过SVD处理后得到的向量不怎么适合作为其他神经网络模型的输入，所以才出现Word2Vec这种方法。Word2Vec使用简单的神经网络模型通过这个词周围的词来表示该词，词表示方式是一个稠密的Distributed Representation（稠密的分散式向量）**

## Word2Vec的前身NNLM模型
NNLM(Nerual Network Language Model)在2003年由徐伟提出，如果理解了NNLM，那么理解Word2Vec中的两个模型就会相对简单，这里就不从数学上进行推导证明了，直接来看一下NNLM模型的结构

![NNLM](http://obfs4iize.bkt.clouddn.com/NNLM.jpg)

这个结构比较简单，从下到上，分别是输入层、映射层、隐藏层和输出层，很常见的神经网络的结构，那么为了更清晰的理解NNLM的过程，先要明白NNLM要做的事情和目标

假设现在有一个语料库，其中有10w个词，那么：

NNLM要做的事情:定义一个滑动窗口，比如这个滑动窗口大小是3，滑动窗口会遍历整个语料库，每次输入3个词，然后预测下个词

结合上面NNLM的模型图,输入的3个词分别就是:
$$w_{t-n+1},w_{t-2},  w_{t-1}$$

预测的词就是：
$$w_t = i|context$$

NNLM的目标：将语料库中的10w个词通过稠密的向量矩阵来表示，这个稠密向量矩阵在NNLM模型图中就是：
$$Matrix C$$

如果你了解神经网络，就会发现NNLM有趣的地方，**一般的神经网络训练数据时通过反向传播算法优化神经网络中的参数，从而让输出层输出我们想要的结果，而NNLM不同，我们最终要的结果是神经网络训练时通过反向传播算法优化的这些参数，这些参数就是语料库中词的词向量**

明白了NNLM要做的事情和目标，接着就来详细讲解NNLM的整个流程

1.滑动窗口获得one-hot形式的词向量输入，NNLM模型中获得3个one-hot形式的词向量
$$w_{t-n+1},w_{t-2},  w_{t-1}$$

2.一开始，随机初始化300*10w的矩阵Matrix C，这就是我们最终需要的词向量，工业界认为，在大的语料库上300维~500维才能表示出词的含义和关系

$$C = (W_1,W_2,....,W_{10w})=
 \left\{
 \begin{matrix}
   (w_1)_1 & (w_2)_1 & ... & (w_{10w})_1 \\
   (w_1)_2 & (w_2)_2 & ... & (w_{10w})_2 \\
   . & . & ... & . \\
   . & . & ... & . \\
   (w_1)_{300} & (w_2)_{300} & ... & (w_{10w})_{300}
  \end{matrix}
  \right\} 
$$

3.Matrix C与one-hot做映射，什么是意思？就是300\*10w的矩阵 与1\*10w的矩阵的转置矩阵相乘，简单说就是 300\*10w与10w\*1的矩阵相乘，结果就是300\*1的矩阵，相当于**将one-hot形式词向量中1的位置对应到300\*10w矩阵的相应位置给切割下来**，这也就是映射层的作用，通过映射，将3个one-hot表示的词向量转为了3个300*1的向量，NNLM模型图中就是
$$C_{(w_{t-n})},C_{(w_{t-2})},C_{(w_{t-1})}$$

4.将映射获得的向量做一个简单链接，这里获得3个300*1的向量，通过链接就可以获得900\*1的向量

5.假设NNLM的隐藏层有500维，那么这里要做的就是将刚刚链接好的900维向量与500维的向量做一个全连接，然后隐藏层再与输出层做一个全连接，从NNLM模型图中可以看出隐藏层与输出层直接使用了tanh双曲正切作为激活函数（这里需要一点神经网络的知识）

6.输出层当然是10w维，它跟语料库是一样大的，最后使用softmax函数对输出层的内容进行处理，将输出层10w维的结果向量变成一个概率分布，那么10w维向量中，概率最大的位置就是要预测的词

7.获得预测结果后与真实结果进行比对，计算预测结果和真实结果的cross entropy loss交叉损失，通过反向传播算法优化该神经网络的参数达到最小化cross entropy loss的目的

8.当我们最小化了cross entropy loss，参数也就是优化完了，一开始随机生成的Matrix C也就有了意义，成为了可以表示这个语料库的稠密词向量矩阵

这就是NNLM模型的整个流程，当然还有很多具体的细节，就不展开讲了

## Word2vec
严格来讲NNLM的效果还是不错的，但是计算量太大了，通过NNLM训练一个语料库耗时太久，为了降低计算量同时保证效果，就提出了word2vec，在word2vec中有两种模型，分别是CBOW(Continuous Bag-of-Words Model)连续词袋模型和Skip-Gram模型(Continuous Skip-Gram Model)

![word2vec模型](http://obfs4iize.bkt.clouddn.com/word2vec%E6%A8%A1%E5%9E%8B.jpg)

从图中可以看出，两个模型的结构是完全相反的，CBOW模型是通过周围的词来预测中间的词，而Skip-Gram模型正好相反，它是通过中间的词来预测周围的词

先来仔细讲讲CBOW，与NNLM模型相比，CBOW的模型结构显得更加简单，它其实就是将NNLM中不必要的计算给去除了，CBOW的流程也比较简单

1.**使用双向的滑动窗口获得词的稠密向量**而不是one-hot表示的词向量，这个稠密向量一开始由我们自己随机初始化，在训练的过程中优化它

2.去掉NNLM中的投影层和隐藏层（因为输入的就是稠密向量了，所以投影层对CBOW没有意义），**对输入层输入的稠密向量进行简单的求和或者求平均**，图中CBOW模型是SUM求和

3.然后再与输出层进行全连接，通过层次的softmax将输出层的数据转换为概率分布，与真实的概率进行对比，**优化CBOW模型的输入，最小化两个概率分析的差距（损失）**

这里有必要提一下层次Softmax(Hierarchical Softmax)，如果要训练10w大小的语料库，输出层就要有10w维，这个维度有点高使得计算量变大，而层次Softmax就是解决这个问题的方法之一，结构如下图

![CBOW.jpg](http://obfs4iize.bkt.clouddn.com/CBOW.jpg)

使用Huffman编码树来编码输出层向量，这样在计算某个词时就只需要计算路径上所有非叶子节点词向量的权重则可，将计算量降为树的深度

除了层次Softmax这个方法，还可以通过负例采样的方式来解决这个问题

因为输出层的维度太高计算复杂，而最终的结果其实只有一个是正确的答案，也就是我们要的词，而其他的都是错误答案，如果语料库是10w，那么输出层就有10w维，在这10w维中，只有1个是正确答案，10w-1个都是错误答案，**在训练是我们不需要这么多错误答案，那么从中抽取一些错误答案组成一个低纬度的向量用来训练模型就好了**，比如只要一个500维的向量进行训练，那么就需要从10w-1个错误答案中抽取出499个错误答案，这些错误答案也叫做负例

随机抽取肯定是不靠谱的，因为在语料库中，每次词出现的频率都不同，所以**要保证频次越高的词应该越容易被采样到**，负例采样也是这个思想，我们将长度为1的线段平均分为M份，不同词频的词所占的比例不同，比如'我'占线段的2/10,'是'占线段的1/10，’帅比‘占线段的3/10，然后上帝就开始投塞子，投到那个就将这个词作为负例取出来，投499次，如果其中选中了正例(正确答案)就重新投

![负例采样](http://obfs4iize.bkt.clouddn.com/%E8%B4%9F%E4%BE%8B%E9%87%87%E6%A0%B7.jpg)

之所以要将线段分为M份，是为了速度，这样每次就不用生成0-1之间的随机数，而生成0-M之间的整数，然后直接去这个刻度上抽一个单词则可，这算是工程实现上的Skill

Skip-Gram就不再细讲，简单来说其实就是讲CBOW反过来了，通过中间的词去预测该词周围的词，还需要提的就是一个工程经验，**在数据量比较小的情况下使用CBOW效果会比较好，而在数据量非常巨大的时候，使用Skip-Gram效果会更好些**

## word2vec的不足
Word2Vec算是NLP中的非常重要的东西，无论你使用什么神经网络，如果你要对语言进行训练，第一步就是对语言进行翻译，将其表示成一个词向量，而Word2Vec就是其中的好手，但是它也有一些不足

1.没有利用语料库全局的信息，从刚刚的模型中也可以看出，word2vec对每个滑动窗口中的词向量进行训练，无法利用全局信息，这点可以去了解 Glove，一个利用了全局信息的模型

2.对多义词无解，因为使用了唯一的词向量，比如Apple，表示的是吃的苹果还是苹果手机

## 结尾
本人能力有限，暂时还无法从数学原理上推到Word2vec，关于这点可以看[word2vec中的数学原理详解](http://blog.csdn.net/itplus/article/details/37969519)

关于CBOW模型和Skip-Gram模型更深点的内容，可以看看[word2vec原理推导与代码分析](http://www.hankcs.com/nlp/word2vec.html)


